{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sd-as/homeworks-modernaicourse/blob/main/hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60ed8c7f",
      "metadata": {
        "id": "60ed8c7f"
      },
      "source": [
        "## Homework 1 - Introduction to Linear Algebra + PyTorch\n",
        "\n",
        "This homework is aimed to familiarize you with some of the basic linear algebra operations we covered in class, as well as how to implement these functions and more in PyTorch.\n",
        "\n",
        "As before, make a copy of the assignment to your drive, add your mugrade key, and then run the cells below to get started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8934a86a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8934a86a",
        "outputId": "4ae762d3-7483-4d39-dcf3-137c0b975997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/locuslab/mugrade.git\n",
            "  Cloning https://github.com/locuslab/mugrade.git to /tmp/pip-req-build-n8lh50d2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/locuslab/mugrade.git /tmp/pip-req-build-n8lh50d2\n",
            "  Resolved https://github.com/locuslab/mugrade.git to commit 717e300a5c2ddc0c729746946f8dc9f0d1c0ecea\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "File ‘hw1_tests.py’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Run this cell to download and installs the necessary modules for the homework\n",
        "!pip install --upgrade --no-deps git+https://github.com/locuslab/mugrade.git\n",
        "!wget -nc https://raw.githubusercontent.com/modernaicourse/hw1/refs/heads/main/hw1_tests.py\n",
        "\n",
        "import os\n",
        "import mugrade\n",
        "import torch\n",
        "\n",
        "from hw1_tests import images, test_classify_zero_one, submit_classify_zero_one, \\\n",
        "    test_vector_add, submit_vector_add, \\\n",
        "    test_vector_inner_product, submit_vector_inner_product, \\\n",
        "    test_matrix_vector_product_1, submit_matrix_vector_product_1, \\\n",
        "    test_matrix_vector_product_2, submit_matrix_vector_product_2, \\\n",
        "    test_vector_matrix_product_2, submit_vector_matrix_product_2, \\\n",
        "    test_matmul_1, submit_matmul_1, \\\n",
        "    test_matmul_2, submit_matmul_2, \\\n",
        "    test_matmul_3, submit_matmul_3, \\\n",
        "    test_batch_matmul, submit_batch_matmul, \\\n",
        "    test_block_matmul, submit_block_matmul\n",
        "\n",
        "os.environ[\"MUGRADE_HW\"] = \"Homework 1\"\n",
        "os.environ[\"MUGRADE_KEY\"] = \"\" ### Your key here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea4a69a0",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ea4a69a0"
      },
      "source": [
        "### Problem 1: \"Classical\" programming for digit classification\n",
        "\n",
        "This course deals primarily with machine learning approaches, but it's worth emphasizing that you _can_ try to approach many of the problems you'll want to solve with machine learning with traditional programming approaches as well.  In this problem, you should experiment with developing a \"manual\" classifier between images of digits in the MNIST dataset, which will be the first machine learning mode you'll develop during the later assignments.  Specifically, you'll want to implement the following function `classify_zero_one` to classify between images of zeros and ones in the MNIST dataset.  Try to think intuitively about features that might distinguish between zeros and ones, and if possible, try not to look at any statistics from the actual dataset (i.e., average values of the images, or anything like that).\n",
        "\n",
        "You can use the `images` dataset loaded above from the `hw1_tests.py` function (specifically the `images.data` and `images.targets` fields, which have been limited to just include the 0/1 images) to help you develop your code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c49a5400",
      "metadata": {
        "id": "c49a5400",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3253b5c4-c7d9-4bf5-a48c-eea123d64883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mugrade: Running local tests for function classify_zero_one():...\n",
            "PASSED\n"
          ]
        }
      ],
      "source": [
        "# you may want to leave this commented until you are ready to test, since it\n",
        "# takes a few seconds to run local tests\n",
        "@mugrade.local_tests\n",
        "def classify_zero_one(image):\n",
        "    \"\"\"\n",
        "    Classify a 28x28 pixel image as either a zero or one.\n",
        "\n",
        "    Input:\n",
        "        image : Tensor - 2D tensor storing grayscale pixel values of the image,\n",
        "                         with each element a real-valued number in [0,1]\n",
        "    Output:\n",
        "        integer : 0 or 1\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    X_train = images.data.reshape(12665, 28*28)\n",
        "    y_train = images.targets\n",
        "    W = torch.zeros(2, 28*28)\n",
        "    for i in range(2):\n",
        "      W[i] = X_train[y_train == i].mean(dim=0)\n",
        "\n",
        "    return (W @ image.reshape(28*28)).argmax().item()\n",
        "    ### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30e8af9d",
      "metadata": {
        "id": "30e8af9d"
      },
      "source": [
        "### Problem 2: Vector Addition\n",
        "\n",
        "In the remainder of this assignment, you're going to implement a wide variety of simple linear algebra operators, _without_ using any of the build-in tensor addition or matrix multiplication operators.  Your code should also throw assertion errors if any of the sizes do not match was it allowed for the given operation (i.e., you should be calling assert() to check that the sizes are correct).  Instead, you should use explicit for loops and element-by-element assignment/operations to implement your function.  You can also create new vectors of the right size as your return variable, etc.\n",
        "\n",
        "First implement a simple vector addition function that adds two vectors together, $x,y \\in \\mathbb{R}^n$.  Note that it is ok if this only works when provided with vectors, i.e., 1D tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "900bdebc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "900bdebc",
        "outputId": "fde517dd-8fdc-40d7-e55a-709fb0dd77ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mugrade: Running local tests for function vector_add():...\n",
            "PASSED\n"
          ]
        }
      ],
      "source": [
        "@mugrade.local_tests\n",
        "def vector_add(x,y):\n",
        "    \"\"\"\n",
        "    Add two vectors x and y, _without_ using the built-in addition of torch.\n",
        "    Instead, you need to manually iterate through the elements of x and y and\n",
        "    add them together. The function should throw an AssertionError, via\n",
        "    calling assert(), if the vectors are not the proper size to add together.\n",
        "\n",
        "    Input:\n",
        "        x : 1D torch.Tensor - first term to add\n",
        "        y : 1D torch.Tensor - second term to add\n",
        "\n",
        "    Output:\n",
        "        return 1D torch.Tensor - sum of x + y\n",
        "\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    assert x.shape == y.shape, \"Vectors must have the same shape\"\n",
        "    result = [i.item() + j.item() for i, j in zip(x, y)]\n",
        "    return torch.tensor(result)\n",
        "    ### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af6bbbc1",
      "metadata": {
        "id": "af6bbbc1"
      },
      "source": [
        "### Problem 3: Vector inner product\n",
        "\n",
        "Now implement the vector inner product.  I.e., for two vectors $x, y \\in \\mathbb{R}^n$, return the inner product\n",
        "$$\\langle x,y \\rangle \\equiv x^T y = \\sum_{i=1}^n x_i y_i.$$\n",
        "\n",
        "As before, don't use any PyTorch functions that compute a matrix multiplication or inner product directly, but do it all with for loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "06b95df1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06b95df1",
        "outputId": "c8ae8787-5268-4794-f895-33481b52bafe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mugrade: Running local tests for function vector_inner_product():...\n",
            "PASSED\n"
          ]
        }
      ],
      "source": [
        "@mugrade.local_tests\n",
        "def vector_inner_product(x,y):\n",
        "    \"\"\"\n",
        "    Compute the inner product between two vectors x and y, _without_ using the\n",
        "    matrix multiplication operator '@' (or any similar PyTorch function). The\n",
        "    function should throw an AssertionError if the vectors are not the proper\n",
        "    size.\n",
        "\n",
        "    Input:\n",
        "        x : 1D torch.Tensor - first term to add\n",
        "        y : 1D torch.Tensor - second term to add\n",
        "\n",
        "    Output:\n",
        "        return float - inner product <x,y>\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    assert x.shape == y.shape\n",
        "    result = [i.item() * j.item() for i, j in zip(x, y)]\n",
        "    return sum(result)\n",
        "    ### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "682888f6",
      "metadata": {
        "id": "682888f6"
      },
      "source": [
        "### Problem 4: Matrix-vector product approach #1\n",
        "\n",
        "Write a routine that computes the matrix-vector product $Ax$ for $A \\in \\mathbb{R}^{m \\times n}$ and $x \\in \\mathbb{R}^n$.  This version should compute each entry of the resuting vector using the inner product between rows of $A$ and the vector $x$, i.e., shown graphically this would be\n",
        "$$\n",
        "Ax = \\left [ \\begin{array}{ccc}\n",
        "\\;\\text{—} & a^T_1 & \\text{—}\\; \\\\\n",
        "\\;\\text{—} & a^T_2 & \\text{—}\\; \\\\\n",
        "& \\vdots & \\\\\n",
        "\\;\\text{—} & a^T_m & \\text{—}\\;\n",
        "\\end{array} \\right ] \\left [ \\begin{array}{c}\\mid \\\\ x \\\\ \\mid \\end{array}  \\right ] = \\left [ \\begin{array}{c} a^T_1 x \\\\ a^T_2 x \\\\ \\vdots \\\\ a^T_m x \\end{array} \\right].\n",
        "$$\n",
        "\n",
        "Only make use of the above-implemented `vector_inner_product()` function you implemetned above for this routine, i.e., no other operations on the tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "eb53e02f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb53e02f",
        "outputId": "118a5dbd-c3ab-43e9-a763-fbfaaeff7aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mugrade: Running local tests for function matrix_vector_product_1():...\n",
            "PASSED\n"
          ]
        }
      ],
      "source": [
        "@mugrade.local_tests\n",
        "def matrix_vector_product_1(A,x):\n",
        "    \"\"\"\n",
        "    Compute the matrix vector product Ax _without_ using the matrix\n",
        "    multiplication operator @ or any related function. In this variant\n",
        "    implement the output as the inner product of each row of A with\n",
        "    the vector x (i.e., only make use of the vector_inner_product function).\n",
        "    Be sure to throw AssertionErrors if the product is not valid.\n",
        "\n",
        "    Input:\n",
        "        A : 2D torch.Tensor - m x n matrix A\n",
        "        x : 1D torch.Tensor - vector x with n elements\n",
        "\n",
        "    Output:\n",
        "        return 1D torch.Tensor - vector Ax with m elements\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    assert A.shape[1] == x.shape[0]\n",
        "    result = [vector_inner_product(A[i], x) for i in range(A.shape[0])]\n",
        "    return torch.tensor(result)\n",
        "    ### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "748084fb",
      "metadata": {
        "id": "748084fb"
      },
      "source": [
        "### Problem 5: Matrix-vector product approach #2\n",
        "\n",
        "Write a routine that computes the matrix-vector product $Ax$ for $A \\in \\mathbb{R}^{m \\times n}$ and $x \\in \\mathbb{R}^n$.  This version should compute the result as a linear combination of the columns of $A$ with coefficients given by the entries of $x_i$, i.e., shows graphically this would be\n",
        "$$\n",
        "Ax = \\left [ \\begin{array}{cccc} \\mid & \\mid & & \\mid \\\\\n",
        "a_1 & a_2 & \\cdots & a_n \\\\\n",
        "\\mid & \\mid & & \\mid \\end{array} \\right ]\n",
        "\\left [ \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{array}\\right ] =\n",
        "\\left [ \\begin{array}{c} \\mid \\\\ a_1 \\\\ \\mid \\end{array} \\right ] x_1 +\n",
        "\\left [ \\begin{array}{c} \\mid \\\\ a_2 \\\\ \\mid \\end{array} \\right ] x_2 + \\ldots +\n",
        "\\left [ \\begin{array}{c} \\mid \\\\ a_n \\\\ \\mid \\end{array} \\right ] x_n\n",
        "$$\n",
        "\n",
        "Only make use of the above-implemented `vector_add()` function to implement your solution (plus of course creating vectors to return, etc).  It is also ok to multiply a vector by a scalar, i.e., the code `c*y` where `c` is a vector and `y` is a real-valued scalar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c67e2139",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c67e2139",
        "outputId": "e6911b4c-8f86-4efa-d2a8-80360f5105fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mugrade: Running local tests for function matrix_vector_product_2():...\n",
            "PASSED\n"
          ]
        }
      ],
      "source": [
        "@mugrade.local_tests\n",
        "def matrix_vector_product_2(A,x):\n",
        "    \"\"\"\n",
        "    Compute the matrix vector product Ax _without_ using the matrix\n",
        "    multiplication operator @ or any related function.  In this variant\n",
        "    implement the output as a linear combination of the columns of A with\n",
        "    coefficients given by the entries of x (and only make use of the\n",
        "    vector_add function). Be sure to throw AssertionErrors if the sizes do\n",
        "    not allow for a valid product\n",
        "\n",
        "    Input:\n",
        "        A : 2D torch.Tensor - m x n matrix A\n",
        "        x : 1D torch.Tensor - vector x with n elements\n",
        "\n",
        "    Output:\n",
        "        return 1D torch.Tensor - vector Ax with m elements\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    assert A.shape[1] == x.shape[0]\n",
        "    result = A[:,0] * x[0]\n",
        "    for i in range(1, A.shape[1]):\n",
        "        result = vector_add(result, A[:,i] * x[i])\n",
        "    return result\n",
        "    ### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c7f8be0",
      "metadata": {
        "id": "9c7f8be0"
      },
      "source": [
        "### Problem 6: Vector-matrix product approach #2\n",
        "\n",
        "Write a routine that function that computes the vector-Matrix product $x^TA$ for $A \\in \\mathbb{R}^{m \\times n}$ and $x \\in \\mathbb{R}^m$. In keeping with PyTorch convention (i.e., not differentiating between column and row vectors), this should return a 1D tensor representing the resulting row vector.  \n",
        "\n",
        "This version should compute the result as a linear combination of the rows of $A$ with coefficients given by the entries of $x_i$, i.e., shows graphically this would be\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "x^T A & =\n",
        "\\left[ \\begin{array}{cccc} x_1 & x_2 & \\ldots & x_m \\end{array} \\right]\n",
        "\\left[ \\begin{array}{ccc}\n",
        "\\;\\text{—} & a^T_1 & \\text{—}\\; \\\\\n",
        "\\;\\text{—} & a^T_2 & \\text{—}\\; \\\\\n",
        "& \\vdots & \\\\\n",
        "\\;\\text{—} & a^T_m & \\text{—}\\;\n",
        "\\end{array} \\right] \\\\ & =\n",
        "x_1 \\left[ \\begin{array}{ccc} \\;\\text{—} & a^T_1 & \\text{—}\\; \\end{array} \\right] +\n",
        "x_2 \\left[ \\begin{array}{ccc} \\;\\text{—} & a^T_2 & \\text{—}\\; \\end{array} \\right] + \\ldots +\n",
        "x_m \\left[ \\begin{array}{ccc} \\;\\text{—} & a^T_m & \\text{—}\\; \\end{array} \\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Only make use of the above-implemented `vector_add()` function to implement your solution, with the same caveats as in the previous problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "10257dd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10257dd9",
        "outputId": "82201b30-2bd0-4c07-9f78-0e5a0743cbf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mugrade: Running local tests for function vector_matrix_product_2():...\n",
            "PASSED\n"
          ]
        }
      ],
      "source": [
        "@mugrade.local_tests\n",
        "def vector_matrix_product_2(x,A):\n",
        "    \"\"\"\n",
        "    Compute the vector Matrix product x^T A _without_ using the matrix\n",
        "    multiplication operator @ or any related function.  In this variant\n",
        "    implement the output as a linear combination of the rows of A with\n",
        "    coefficients given by the entries of x (and only make use of the\n",
        "    vector_add function).  Note that, in keeping with PyTorch convention (of\n",
        "    not differentiating between row and column vectors), x will just be an\n",
        "    vector (1D tensor) with m elements, and the output should be a vector (1D\n",
        "    Tensor) with n elements. Be sure to throw AssertionErrors if the sizes do\n",
        "    not allow for a valid product.\n",
        "\n",
        "    Input:\n",
        "        A : 2D torch.Tensor - m x n matrix A\n",
        "        x : 1D torch.Tensor - vector x with m elements\n",
        "\n",
        "    Output:\n",
        "        return 1D torch.Tensor - vector x^T A with n elements\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    assert  x.shape[0] == A.shape[0]\n",
        "    result =  x[0] * A[0]\n",
        "    for i in range(1, A.shape[0]):\n",
        "        result = vector_add(result, x[i] * A[i] )\n",
        "    return result\n",
        "    ### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4788d725",
      "metadata": {
        "id": "4788d725"
      },
      "source": [
        "### Problem 7: Matrix-matrix multiplication approach #1\n",
        "\n",
        "Write a matrix-matrix multiplication function, again without using any built-in operators.  For $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$, this version should compute each element $(AB)_{ij}$ as the inner product of the $i$th row of $A$ and the $j$th column of $B$. Depicted graphically, this would be the breakdown\n",
        "\n",
        "$$\n",
        "AB =\n",
        "\\left[ \\begin{array}{ccc}\n",
        "\\;\\text{—} & a^T_1 & \\text{—}\\; \\\\\n",
        "\\;\\text{—} & a^T_2 & \\text{—}\\; \\\\\n",
        "& \\vdots & \\\\\n",
        "\\;\\text{—} & a^T_m & \\text{—}\\;\n",
        "\\end{array} \\right]\n",
        "\\left[ \\begin{array}{cccc}\n",
        "| & | & & | \\\\\n",
        "b_1 & b_2 & \\cdots & b_p \\\\\n",
        "| & | & & |\n",
        "\\end{array} \\right]\n",
        "=\n",
        "\\left[ \\begin{array}{cccc}\n",
        "a_1^T b_1 & a_1^T b_2 & \\cdots & a_1^T b_p \\\\\n",
        "a_2^T b_1 & a_2^T b_2 & \\cdots & a_2^T b_p \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_m^T b_1 & a_m^T b_2 & \\cdots & a_m^T b_p\n",
        "\\end{array} \\right]\n",
        "$$\n",
        "\n",
        "With all the same caveats as before, this implementation should only use the function `vector_inner_product()` that you implemented above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3a29c8e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a29c8e7",
        "outputId": "5a340ed3-5dc6-4b62-80ef-fe52eab1a4e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mugrade: Running local tests for function matmul_1():...\n",
            "PASSED\n"
          ]
        }
      ],
      "source": [
        "@mugrade.local_tests\n",
        "def matmul_1(A,B):\n",
        "    \"\"\"\n",
        "    Compute the matrix matrix multiplication AB without using the @ operator.\n",
        "    In this variant, compute each entry of the matrix product as the inner\n",
        "    product of a row of A and a column of B (i.e., using the\n",
        "    vector_inner_product function). Be sure to throw AssertionErrors if the\n",
        "    sizes of the matrices do not make for a valid product.\n",
        "\n",
        "\n",
        "    Input:\n",
        "        A : 2D torch.Tensor - m x n matrix A\n",
        "        B : 2D torch.Tensor - n x p matrix B\n",
        "\n",
        "    Output:\n",
        "        return 2D torch.Tensor - m x p matrix equal to the product AB\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    assert A.shape[1] == B.shape[0]\n",
        "    return torch.tensor([ [vector_inner_product(A[i], B[:,j]) for j in range(B.shape[1])] for i in range(A.shape[0]) ])\n",
        "    ### END YOUR CODE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a79331d",
      "metadata": {
        "id": "9a79331d"
      },
      "source": [
        "### Problem 8: Matrix-matrix multiplication approach #2\n",
        "\n",
        "Write another matrix multiplication implemention. For $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$, this version should compute the $i$th column of $AB$ as the matrix-vector product between $A$ and $i$th column of $B$. Depicted graphically, this would be the breakdown\n",
        "$$\n",
        "AB =\n",
        "A\n",
        "\\left [ \\begin{array}{cccc} \\mid & \\mid & & \\mid \\\\\n",
        "b_1 & b_2 & \\cdots & b_p \\\\\n",
        "\\mid & \\mid & & \\mid \\end{array} \\right ]\n",
        "=\n",
        "\\left [ \\begin{array}{cccc} \\mid & \\mid & & \\mid \\\\\n",
        "A b_1 & A b_2 & \\cdots & A b_p \\\\\n",
        "\\mid & \\mid & & \\mid \\end{array} \\right ]\n",
        "$$\n",
        "\n",
        "With all the same caveats as before, this implementation should only use the function `matrix_vector_product_1()` (or `matrix_vector_product_2()`) that you implemented above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "94a324d3",
      "metadata": {
        "id": "94a324d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a1c6c4e-1b12-40c9-db0a-1b7bff190202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mugrade: Running local tests for function matmul_2():...\n",
            "PASSED\n"
          ]
        }
      ],
      "source": [
        "@mugrade.local_tests\n",
        "def matmul_2(A,B):\n",
        "    \"\"\"\n",
        "    Compute the matrix matrix multiplication AB without using the @ operator.\n",
        "    In this variant, compute the ith _column_ of the matrix product as the\n",
        "    matrix-vector product of A and the ith column of B (i.e., using only the\n",
        "    function matrix_vector_product_1 or matrix_vector_product_2). Be sure to\n",
        "    throw AssertionErrors if the sizes of the matrices do not make for a valid\n",
        "    product.\n",
        "\n",
        "    Input:\n",
        "        A : 2D torch.Tensor - m x n matrix A\n",
        "        B : 2D torch.Tensor - n x p matrix B\n",
        "\n",
        "    Output:\n",
        "        return 2D torch.Tensor - m x p matrix equal to the product AB\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    assert A.shape[1] == B.shape[0]\n",
        "    return torch.stack([matrix_vector_product_1(A, B[:,i]) for i in range(B.shape[1])], dim = 1)\n",
        "\n",
        "    ### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc75eb6c",
      "metadata": {
        "id": "fc75eb6c"
      },
      "source": [
        "### Problem 9: Matrix-matrix multiplication approach #3\n",
        "\n",
        "Finally, write one last matrix multiplication implementation. For $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$, this version should compute the $i$th row of $AB$ as the vector-matrix product between the $i$th row of $A$ and $B$. This would be the breakdown\n",
        "$$\n",
        "AB =\n",
        "\\left [ \\begin{array}{ccc}\n",
        "\\;\\text{—} & a^T_1 & \\text{—}\\; \\\\\n",
        "\\;\\text{—} & a^T_2 & \\text{—}\\; \\\\\n",
        "& \\vdots & \\\\\n",
        "\\;\\text{—} & a^T_m & \\text{—}\\;\n",
        "\\end{array} \\right ] B =\n",
        "\\left [ \\begin{array}{ccc}\n",
        "\\;\\text{—} & a^T_1 B & \\text{—}\\; \\\\\n",
        "\\;\\text{—} & a^T_2 B & \\text{—}\\; \\\\\n",
        "& \\vdots & \\\\\n",
        "\\;\\text{—} & a^T_m B & \\text{—}\\;\n",
        "\\end{array} \\right ]\n",
        "$$\n",
        "\n",
        "\n",
        "With all the same caveats as before, this implementation should only use the function `vector_matrix_product_2()` that you implemented above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "6c89ddc9",
      "metadata": {
        "id": "6c89ddc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac92b9e1-61b5-47bf-a6ca-c3fad9ef828c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mugrade: Running local tests for function matmul_3():...\n",
            "PASSED\n"
          ]
        }
      ],
      "source": [
        "@mugrade.local_tests\n",
        "def matmul_3(A,B):\n",
        "    \"\"\"\n",
        "    Compute the matrix matrix multiplication AB without using the @ operator.\n",
        "    In this variant, compute the ith _row_ of the matrix product as the\n",
        "    vector-matrix product of the ith row A and B (i.e., using only the\n",
        "    function vector_matrix_product_2). Be sure to throw AssertionErrors if the\n",
        "    sizes of the matrices do not make for a valid product.\n",
        "\n",
        "    Input:\n",
        "        A : 2D torch.Tensor - m x n matrix A\n",
        "        B : 2D torch.Tensor - n x p matrix B\n",
        "\n",
        "    Output:\n",
        "        return 2D torch.Tensor - m x p matrix equal to the product AB\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    assert A.shape[1] == B.shape[0]\n",
        "\n",
        "    return torch.stack([vector_matrix_product_2(A[i], B) for i in range(A.shape[0])])\n",
        "\n",
        "    ### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b1139fd",
      "metadata": {
        "id": "6b1139fd"
      },
      "source": [
        "### Problem 10: Batch matrix multiplication\n",
        "\n",
        "In this problem, you will implement batch matrix multiplication.  Consider two ND PyTorch tensors of the dimensions $A \\in \\mathbb{R}^{n_1 \\times n_2 \\times \\ldots \\times n_d}$ $B \\in \\mathbb{R}^{m_1 \\times m_2 \\times \\ldots \\times m_d}$ with the same sizes on all but the last two dimensions\n",
        "$$ n_i = m_i, \\; i=1,\\ldots,d-2$$\n",
        "and the last two dimensions properly sized for a matrix multiplication\n",
        "$$n_i = m_{i-1}.$$\n",
        "In this case implement a batched version of matrix multiplication that iterates over all the leading $d-2$ dimensions and performs a matrix multiplication of the corresponding entries.  The function should throw an AssertionError if any of the sizes do not match.\n",
        "\n",
        "You should still not use the PyTorch matrix multiplication operator, but instead call one of the `matmul()` functions you implemented above (it doesn't really matter which one)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed4abb76",
      "metadata": {
        "id": "ed4abb76"
      },
      "outputs": [],
      "source": [
        "@mugrade.local_tests\n",
        "def batch_matmul(A,B):\n",
        "    \"\"\"\n",
        "    Implement batch matrix multiplication between 2 tensors A and B by\n",
        "    iterating over all the leading dimensions of A and B (all dimensions other\n",
        "    than the last two), and performing a matrix multiplication over the last\n",
        "    two dimensions. A and B must be sized so that their leading dimensions are\n",
        "    all the same, and the last two dimensions are sized for a valid matrix\n",
        "    multiplication.\n",
        "\n",
        "    Inputs:\n",
        "        A : torch.Tensor - ND tensor with trailing dimensions (..., m, n)\n",
        "        B : torch.Tensor - ND tensor with trailing dimensions (..., n, p)\n",
        "\n",
        "    Output:\n",
        "        return torch.Tensor - ND tensor with tailing dimensions (..., m, p)\n",
        "                              containing all matrix multiplications of the\n",
        "                              corresponding entries.\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    pass\n",
        "\n",
        "    ### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b998e7d",
      "metadata": {
        "id": "4b998e7d"
      },
      "source": [
        "### Problem 11: Block matrix multiplication\n",
        "In this last question, you'll implement a \"blocked\" form of matrix multiplication.  Although we defined matrix multiplication in terms of the individual scalar entries of a matrix, it can also be defined by operating on subblocks of the matrices.  Specifically for an matrix $A \\in \\mathbb{R}^{4m \\times 4n}$ we can define $A_{ij} \\in \\mathbb{R}^{4 \\times 4}$ to be a _subblock_ of the matrix, and similarly for the matrix $B \\in \\mathbb{R}^{4n \\times 4p}$.  Then the corresponding $4 \\times 4$ subblock of the matrix product $AB$ can be computed as\n",
        "$$ (AB)_{ij} = \\sum_{k=1}^n A_{ik} B_{kj} $$\n",
        "analogous to the usual definition of matrix multiplication, but with $A_{ik} B_{kj}$ now being a matrix product.\n",
        "\n",
        "In practice, techniques like this (with proper memory layouts, which we don't cover here) are how one writes fast matrix multiplication primitives on GPUs (where e.g., so-called \"tensor cores\" actually exactly perform 4x4 matrix multiplication).\n",
        "\n",
        "Implement the `block_matmul` function below.  You should _only_ call the `add_matmul_44()` function in your implementation. You should check to ensure that the matrices form a valid matrix multiplication, and that they are all divisible by 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1f8b83a",
      "metadata": {
        "id": "b1f8b83a"
      },
      "outputs": [],
      "source": [
        "def add_matmul_44(Z,A,B):\n",
        "    \"\"\"\n",
        "    Simulate a \"fast\" 4x4 matrix multiplication and in-place addition to Z:\n",
        "        Z += AB\n",
        "    \"\"\"\n",
        "    assert(Z.shape == (4,4) and A.shape == (4,4) and B.shape == (4,4))\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            Z[i,j] += A[i,0]*B[0,j] + A[i,1]*B[1,j] + A[i,2]*B[2,j] + A[i,3]*B[3,j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03592d75",
      "metadata": {
        "id": "03592d75"
      },
      "outputs": [],
      "source": [
        "@mugrade.local_tests\n",
        "def block_matmul(A,B):\n",
        "    \"\"\"\n",
        "    Implement a block matrix multiplication to compute the matrix-matrix\n",
        "    product AB.  You should use the formula above, and also assert that that\n",
        "    matrices are the proper shapes (and have dimensions that are multiples of\n",
        "    4). Use only the matmul_44 call.\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    pass\n",
        "    ### END YOUR CODE\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "15780",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}