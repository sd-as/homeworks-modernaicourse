{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7082797e",
      "metadata": {
        "id": "7082797e"
      },
      "source": [
        "## Homework 2 - Automatic differentiation and linear models\n",
        "\n",
        "This homework contains two main portions. In part one, you will implement an extremely minimal automatic differentiation module.  This is the same technique that underlies PyTorch, and while you will not implement anything close to the complexity of a library like PyTorch, it will give you a basic understanding of the basic principles of the approach, giving you some insight into how the nuts and bolts of PyTorch do work under the hood.  In the second part, you will use the (built in) automatic differentiation tooling of PyTorch to train a simple linear model; note that for this assignment you won't do this in the \"normal\" PyTorch way of defining PyTorch Module subclasses, optimizer subclasses, and this sort of thing (that will be done in the next homework, on neural networks), but you will use the basic gradient descent approach to build a simple linear model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "299b49ed",
      "metadata": {
        "id": "299b49ed",
        "outputId": "055405c2-614f-4816-ae1f-1c624eae3ed9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/locuslab/mugrade.git\n",
            "  Cloning https://github.com/locuslab/mugrade.git to /tmp/pip-req-build-qwxe5z4b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/locuslab/mugrade.git /tmp/pip-req-build-qwxe5z4b\n",
            "  Resolved https://github.com/locuslab/mugrade.git to commit 717e300a5c2ddc0c729746946f8dc9f0d1c0ecea\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "File ‘hw2_tests.py’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Run this cell to download and installs the necessary modules for the homework\n",
        "!pip install --upgrade --no-deps git+https://github.com/locuslab/mugrade.git\n",
        "!wget -nc https://raw.githubusercontent.com/modernaicourse/hw2/refs/heads/main/hw2_tests.py\n",
        "\n",
        "import os\n",
        "import mugrade\n",
        "import math\n",
        "from hw2_tests import (test_Add, submit_Add,\n",
        "                       test_Subtract, submit_Subtract,\n",
        "                       test_Divide, submit_Divide,\n",
        "                       test_Power, submit_Power,\n",
        "                       test_Log, submit_Log,\n",
        "                       test_Exp, submit_Exp,\n",
        "                       test_compute_gradients, submit_compute_gradients,\n",
        "                       test_cross_entropy_loss, submit_cross_entropy_loss,\n",
        "                       test_error, submit_error,\n",
        "                       test_train_sgd, submit_train_sgd)\n",
        "\n",
        "os.environ[\"MUGRADE_HW\"] = \"Homework 2\"\n",
        "os.environ[\"MUGRADE_KEY\"] = \"\" ### Your key here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff04a1df",
      "metadata": {
        "id": "ff04a1df"
      },
      "source": [
        "## Part I - Automatic differentiation\n",
        "\n",
        "At the core of automatic differentiation is a technique that builds a _compute graph_, which constructs a graph out of a series of functions applied to variables.  In our setting, we will implement this functionality with two simple classes: a `Variable` class that represents the variables we will differentiate with respect to and a `Function` class that contains the logic to both implement the function itself and compute its gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac1aa3dd",
      "metadata": {
        "id": "ac1aa3dd"
      },
      "outputs": [],
      "source": [
        "class Variable:\n",
        "    def __init__(self, value, function = None, parents=None):\n",
        "        \"\"\"\n",
        "        Initialize the variable with its needed properties.\n",
        "        \"\"\"\n",
        "        self.value = value\n",
        "        self.grad = None\n",
        "        self.function = function\n",
        "        self.parents = parents\n",
        "        self.num_children = 0\n",
        "\n",
        "    ### these functions will call later implementations you develop\n",
        "    def __repr__(self): return f\"Variable({self.value}, grad={self.grad})\"\n",
        "    def __add__(self, other): return Add()(self, other)\n",
        "    def __sub__(self, other): return Subtract()(self, other)\n",
        "    def __mul__(self, other): return Multiply()(self, other)\n",
        "    def __truediv__(self, other): return Divide()(self, other)\n",
        "    def __neg__(self): return Negate()(self)\n",
        "    def __pow__(self, d): return Power(d)(self)\n",
        "    def log(self): return Log()(self)\n",
        "    def exp(self): return Exp()(self)\n",
        "\n",
        "class Function:\n",
        "    def __call__(self, *args):\n",
        "        \"\"\"\n",
        "        Construct a node in the computation graph via calling the function\n",
        "        \"\"\"\n",
        "        value = self.forward(*[a.value for a in args])\n",
        "        for p in args:\n",
        "            p.num_children += 1\n",
        "        return Variable(value, function=self, parents=args)\n",
        "\n",
        "    def forward(self, input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, grad, input):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed434a0e",
      "metadata": {
        "id": "ed434a0e"
      },
      "source": [
        "Let's discuss in some detail how the automatic differentiation works, which is best understood by an example.  Let's consider the following operation for two variables `x` and `y`:\n",
        "```python\n",
        "(x*y + x**2)/y\n",
        "```\n",
        "To be even more explicit, let's assign names to all the intermediate terms generated by this computation\n",
        "```python\n",
        "a = x*y\n",
        "b = x**2\n",
        "c = a + b\n",
        "d = c / y\n",
        "```\n",
        "Such an operation would correspond to the following computation graph:\n",
        "\n",
        "![computation graph](https://raw.githubusercontent.com/modernaicourse/hw2/refs/heads/main/computation_graph.svg)\n",
        "\n",
        "In this graph, the original variables and intermediate terms are represented as nodes, and the parents of a node represent the variables that were used to compute that term.  Although not depicted in the graph, each variable in the graph also stores a link to the function that created that variable (as a function of its parents).\n",
        "\n",
        "In our code, these computations graphs are modeled implicitly via the `Variable` class.  Specifically, the class contains the following items:\n",
        "  - `.value` : a `float` value that contain the numerical value of the variable\n",
        "  - `.grad` : a `float` value (or `None`) that will be populated with the variable's derivative with respect to a final function\n",
        "  - `.parents` : the parents of the node in the graph or `None` if the node has no parents\n",
        "  - `.function` : a reference to the function that was used to create the node from its parents (which will be a reference to an instance of the `Function` class)\n",
        "  - `.num_children` : the number of children that each node has (this will be needed for counting whether all children have already computed their gradient, we could compute it online, but this would make the code more complex)\n",
        "\n",
        "In addition to the `Variable` class, there is also a `Function` class that creates variables in a fashion that builds the graph.  Specifically, the `__call__` method of the class implements the routine that constructs the graph. This lets you call a function like multiplication in the following manner:\n",
        "```python\n",
        "Multiply()(x,y)\n",
        "```\n",
        "which initializes the `Multiply()` class and then calls the resulting with arguments `x` and `y`, which invokes the `__call__()` function.\n",
        "\n",
        "Subclasses of `Function` need to implement two methods:\n",
        "  1. The `.forward()` method actually computes the function.  For instance, the forward method of a `Multiply` class would multiply two numbers together, the forward pass of the `Log` class would take the log of a variable, etc.  As you see from the implementation above, this forward call is called by the `__call__()` class, but with additional code that constructions the graph.\n",
        "  2. The `.backward()` function computes the _product_ of what's referred to as an \"incoming derivative\" term (this will correspond to the already-computed derivative of nodes later in the graph), and the _partial derivatives_ of this function.  In general, the arguments to the backward function will always be both this incoming derivative and the arguments to the original function.  For example, if we consider some function of two variables $f(x,y)$, and incoming derivative $g \\in \\mathbb{R}$, then the `.backward()` function would compute two separate product of partial derivatives, which are returned as a list.\n",
        "  $$ \\frac{\\partial f(x,y)}{\\partial x} \\cdot g, \\;\\; \\frac{\\partial f(x,y)}{\\partial y} \\cdot g $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cee82795",
      "metadata": {
        "id": "cee82795"
      },
      "source": [
        "### Example: multiplication\n",
        "Let's see how these look in a few examples.  First let's show the implementation of a `Multiply` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33644626",
      "metadata": {
        "id": "33644626"
      },
      "outputs": [],
      "source": [
        "class Multiply(Function):\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        Compute the forward pass, in this case multiplying x and y\n",
        "        Input:\n",
        "            x: float - first argument\n",
        "            y: float - second argument\n",
        "        Output:\n",
        "            float - equal to x * y\n",
        "        \"\"\"\n",
        "        return x*y\n",
        "\n",
        "    def backward(self, grad, x, y):\n",
        "        \"\"\"\n",
        "        Compute the product of grad and each partial derivative of the function.\n",
        "        Input:\n",
        "            grad: float - incoming derivative\n",
        "            x: float - first argument (to the original forward call)\n",
        "            y: float - second argument (to the original forward call)\n",
        "        Output:\n",
        "            list[float] - list of floats for the products of grad and each\n",
        "                          partial derivative of the function\n",
        "        \"\"\"\n",
        "        return [y*grad, x*grad]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f84b50d",
      "metadata": {
        "id": "3f84b50d"
      },
      "source": [
        "In this case, we are defining the function\n",
        "$$f(x,y) = x y.$$\n",
        "The `.forward()` function simply implements this multiplication, `x*y`.  Furthermore, the partial derivatives of this particular function are given by\n",
        "$$\\frac{\\partial f(x,y)}{\\partial x} = y, \\;\\; \\frac{\\partial f(x,y)}{\\partial y} = x.$$\n",
        "Thus the `.backward()` function computes the product of the incoming derivative `grad` and each of these partial derivatives, and return them as a list."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1680618b",
      "metadata": {
        "id": "1680618b"
      },
      "source": [
        "### Example: Negation\n",
        "\n",
        "Let's look at one more example, this time a function with a single argument, given by $f(x) = -x$.  In this case, the partial derivative is given by\n",
        "$$\\frac{\\partial f(x)}{\\partial x} = -1$$\n",
        "so the `.backward()` function returns `[-grad]` (note that the backward function _always_ returns a list, in this case of just one element, even if the function has only one argument)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "574a3a7e",
      "metadata": {
        "id": "574a3a7e"
      },
      "outputs": [],
      "source": [
        "class Negate(Function):\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the forward pass, in this case negating x\n",
        "        Input:\n",
        "            x : float - argument to function\n",
        "        Output:\n",
        "            return float - negation of x\n",
        "        \"\"\"\n",
        "        return -x\n",
        "\n",
        "    def backward(self, grad, x):\n",
        "        \"\"\"\n",
        "        Compute product of incoming derivative grad and partial derivative\n",
        "        Input:\n",
        "            grad: float - incoming derivative\n",
        "            x: float - argument (to the original forward call)\n",
        "        Output:\n",
        "            list[float] - list of a single float for the product of grad and\n",
        "                          partial derivative of the function\n",
        "        \"\"\"\n",
        "        return [-grad]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44955894",
      "metadata": {
        "id": "44955894"
      },
      "source": [
        "### Question 1 - Function implementations\n",
        "\n",
        "Implement the following `Function` classes to complete the implementation of the operators listed in the `Variable` class.  We are just providing the class definition itself: you'll need to define and implement `.forward()` and `.backward()` functions in each of these.  Remember that `.backward()` needs to always return a _list_ of products between the incoming derivative and each partial derivative, even if there is only a single argument.\n",
        "\n",
        "The one slightly-less straightforward implementation here is the `Power` class, which computes the operation `x**d`.  In this case, we won't actually enable differentiation with respect to the `d` variable (we certainly could, it's just a slightly more involved implementation function, so we don't do it in this assignment).  Thus, for this implementation, we'll store the `d` value in the class itself, and pass it to the `__init__()` operation of the function.  This is what's done in the `Variable` class above, i.e., whereas we call the `Multiply` class like the following:\n",
        "```python\n",
        "Multiply()(x,y)\n",
        "```\n",
        "(i.e., we initialize the class, then call it with the `x` and `y` arguments).  You would call `Power` via the following:\n",
        "```python\n",
        "Power(d)(x)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e35c6934",
      "metadata": {
        "id": "e35c6934"
      },
      "outputs": [],
      "source": [
        "@mugrade.local_tests\n",
        "class Add(Function):\n",
        "    \"\"\"\n",
        "    Implements addition between two variables f(x,y) = x + y\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    pass\n",
        "    ### END YOUR CODE\n",
        "\n",
        "@mugrade.local_tests\n",
        "class Subtract(Function):\n",
        "    \"\"\"\n",
        "    Implements subtraction between two variables f(x,y) = x - y\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    pass\n",
        "    ### END YOUR CODE\n",
        "\n",
        "@mugrade.local_tests\n",
        "class Divide(Function):\n",
        "    \"\"\"\n",
        "    Implements division between two variables f(x,y) = x / y\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    pass\n",
        "    ### END YOUR CODE\n",
        "\n",
        "@mugrade.local_tests\n",
        "class Power(Function):\n",
        "    \"\"\"\n",
        "    Implements the power function between two variables f(x) = x^d.  Since the\n",
        "    function does _not_ need to provide a derivative with respect to the the\n",
        "    d argument, you should instead implement an __init__ function that stores\n",
        "    the d variable as a member, and use this in the forward pass.  The final\n",
        "    usage of the class will then be what is done by our `Variable`\n",
        "    implementation above.\n",
        "\n",
        "    Be sure to handle the case where d is zero (i.e., the function is equal to\n",
        "    one).\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    pass\n",
        "    ### END YOUR CODE\n",
        "\n",
        "@mugrade.local_tests\n",
        "class Log(Function):\n",
        "    \"\"\"\n",
        "    Implements the (natural) logarithm of a function f(x) = log(x).  You can\n",
        "    use calls from the math package to implement this.\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    pass\n",
        "    ### END YOUR CODE\n",
        "\n",
        "@mugrade.local_tests\n",
        "class Exp(Function):\n",
        "    \"\"\"\n",
        "    Implements the exponential (with base e) of x, f(x) = e^x.  You can use\n",
        "    calls from the math package to implement this.\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    pass\n",
        "    ### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f41bcc9a",
      "metadata": {
        "id": "f41bcc9a"
      },
      "source": [
        "If you implement all the following arguments above, you should be able to run the following code that will implicitly build a compute graph of the following expression we described above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93f1a6c1",
      "metadata": {
        "id": "93f1a6c1",
        "outputId": "fef05b8e-ced5-4fcf-e43b-b354afa8a43f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable(4.8, grad=None)\n"
          ]
        }
      ],
      "source": [
        "x = Variable(3.0)\n",
        "y = Variable(5.0)\n",
        "d = (x*y + x**2)/y\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f96b3644",
      "metadata": {
        "id": "f96b3644"
      },
      "source": [
        "### Question 2 - Implementing the full backward pass\n",
        "\n",
        "Given the functions defined above, now you'll implement the actual automatic differentiation pass that will extend the Variable class to compute gradients in a computation graph.  This is done with a `compute_gradients()` function called on the `Variable` instance [Note: in PyTorch this function is also called `.backward()`, but that can be a bit confusing since the method in `Function` is called the same thing, so I want to differentiate.]\n",
        "\n",
        "The basic algorithm is quite simple, and we'll outline it here, then explain some of the intuition behind it. Note that this implementation makes very simplifying assumptions (for instance, it requires that variables not be used anywhere _except_ in the computation of the final output we are differentiating, or their `num_children` count will never reach zero), and it doesn't for example, let you compute gradients multiple times (because it actively manipulates the `num_children` counters) but it nonetheless works for our simple cases.  The algorithm is as follows:\n",
        "\n",
        "#### Backward pass `compute_gradients()` called on `Variable` instance:\n",
        "1. If the `grad` variable of node is `None` (i.e., this is the final function being differentiated), set to 1.0.\n",
        "2. If the node has `parents` and `function` values (i.e., it is a computed node):\n",
        "    1. Call the function's `backward()` implementation passing the node's `grad` value and the parent's values. This returns a list of products `grad` and the partial derivatives of the function.\n",
        "    2. For each of the node's parents:\n",
        "        - Add the corresponding grad/partial derivative product to the parent's `.grad` property (or set it if the `grad` is currently None).\n",
        "        - Decrease the `num_children` parameter of the parent\n",
        "        - If the parent's `num_children` is zero, call its `.compute_gradients()` method recursively.\n",
        "\n",
        "\n",
        "### Worked-through example\n",
        "\n",
        "Let's see how this works in a slightly different version of our example above (we'll ignore that last computation step, just to keep things simpler).  Note that going through all of this at first may not be needed, but it can be helpful to go through as you debug your implementation.\n",
        "```python\n",
        "a = x*y\n",
        "b = x**2\n",
        "c = a + b\n",
        "```\n",
        "After we construct the computation graph, say with values `x=3.0`, `y=4.0`, it would have the following values\n",
        "```python\n",
        "x = Variable(value = 3.0, grad=None, parents = None, function = None, num_children=2)\n",
        "y = Variable(value = 4.0, grad=None, parents = None, function = None, num_children=1)\n",
        "a = Variable(value = 12.0, grad=None, parents = [x,y], function = Multiply, num_children=1)\n",
        "b = Variable(value = 9.0, grad=None, parents = [x], function = Power, num_children=1)\n",
        "c = Variable(value = 21.0, grad = None, parents = [a,b], function = Add, num_children=0)\n",
        "```\n",
        "We would call `compute_gradients()` on `c`, which would first set `c.grad=1.0`, corresponding to the simple fact that\n",
        "$$\\frac{\\partial c}{\\partial c} = 1.$$\n",
        "This would then call:\n",
        "```python\n",
        "grad_partials_products = c.function.backward(c.grad, a.value, b.value) # = [1, 1]\n",
        "```\n",
        "and set `a.grad` and `b.grad` to each of these values, which represents the fact that\n",
        "$$\\frac{\\partial c}{\\partial a} = 1, \\frac{\\partial c}{\\partial b} = 1$$\n",
        "and decrease the `num_children` parameter of `a` and `b`. These last three nodes would now take on the values\n",
        "```python\n",
        "a = Variable(value = 12.0, grad=1.0, parents = [x,y], function = Multiply, num_children=0)\n",
        "b = Variable(value = 9.0, grad=1.0, parents = [x], function = Power, num_children=0)\n",
        "c = Variable(value = 21.0, grad = 1.0, parents = [a,b], function = Add, num_children=0)\n",
        "```\n",
        "Since the parents of `c` both have `num_children=0`, the function would then call `.compute_gradients()` recursively on each of these nodes.  Let's consider calling `a.compute_gradients()` first.  This would compute\n",
        "```python\n",
        "grad_partials_products = a.function.backward(a.grad, x.value, y.value) # = [1*4.0, 1*3.0]\n",
        "```\n",
        "which after similar updates would result in the values\n",
        "```python\n",
        "x = Variable(value = 3.0, grad=4.0, parents = None, function = None, num_children=1)\n",
        "y = Variable(value = 4.0, grad=3.0, parents = None, function = None, num_children=0)\n",
        "```\n",
        "Finally, calling `b.compute_gradients()`, would compute\n",
        "```python\n",
        "grad_partials_products = b.function.backward(b.grad, x.value) # = [2*3.0]\n",
        "```\n",
        "and result in the final `x` variable\n",
        "```python\n",
        "x = Variable(value = 3.0, grad=10.0, parents = None, function = None, num_children=0)\n",
        "y = Variable(value = 4.0, grad=3.0, parents = None, function = None, num_children=0)\n",
        "```\n",
        "which contains all the correct gradients.\n",
        "\n",
        "\n",
        "### Your implementation\n",
        "\n",
        "Implement the `compute_gradients` method of the following `Variable` class as described above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c8be991",
      "metadata": {
        "id": "7c8be991"
      },
      "outputs": [],
      "source": [
        "class Variable:\n",
        "    def __init__(self, value, function = None, parents=None):\n",
        "        self.value = value\n",
        "        self.grad = None\n",
        "        self.function = function\n",
        "        self.parents = parents\n",
        "        self.num_children = 0\n",
        "\n",
        "    ### these functions call the implementations above\n",
        "    def __repr__(self): return f\"Variable({self.value}, grad={self.grad})\"\n",
        "    def __add__(self, other): return Add()(self, other)\n",
        "    def __sub__(self, other): return Subtract()(self, other)\n",
        "    def __mul__(self, other): return Multiply()(self, other)\n",
        "    def __truediv__(self, other): return Divide()(self, other)\n",
        "    def __neg__(self): return Negate()(self)\n",
        "    def __pow__(self, d): return Power(d)(self)\n",
        "    def log(self): return Log()(self)\n",
        "    def exp(self): return Exp()(self)\n",
        "\n",
        "    @mugrade.local_tests\n",
        "    def compute_gradients(self):\n",
        "        \"\"\"\n",
        "        Recursively compute derivatives in a computation graph.  This method\n",
        "        iteratively computes the gradients for a node and all it's parents.\n",
        "        It has no input or output arguments, but instead directly modifies\n",
        "        the `Variable` objects, populating the `.grad` variables as needed\n",
        "        and calling the function recursively on its parents as needed.\n",
        "        \"\"\"\n",
        "        ### BEGIN YOUR CODE\n",
        "        pass\n",
        "        ### END YOUR CODE\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce86dd95",
      "metadata": {
        "id": "ce86dd95"
      },
      "source": [
        "If your implementation is correct, you can now run commands like the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4070d884",
      "metadata": {
        "id": "4070d884",
        "outputId": "4a30b25d-3859-4589-c60f-f751cb2612f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable(3.0, grad=2.5) Variable(4.0, grad=-0.5625)\n"
          ]
        }
      ],
      "source": [
        "x = Variable(3.0)\n",
        "y = Variable(4.0)\n",
        "((x*y + x**2)/y).compute_gradients()\n",
        "print(x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdb9cc36",
      "metadata": {
        "id": "cdb9cc36"
      },
      "source": [
        "Or more complex examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f0eab61",
      "metadata": {
        "id": "6f0eab61",
        "outputId": "0a4452d7-4f27-481f-947e-63786fb14b95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable(3.0, grad=0.0800177178356464) Variable(4.0, grad=0.024005315350693918) Variable(1.2, grad=-2.3710150063849387)\n"
          ]
        }
      ],
      "source": [
        "x = Variable(3.0)\n",
        "y = Variable(4.0)\n",
        "z = Variable(1.2)\n",
        "((x*y + x**2 + z).log()/(z**3).exp()).compute_gradients()\n",
        "print(x,y,z)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5580edf",
      "metadata": {
        "id": "e5580edf"
      },
      "source": [
        "## Part II - Training a digit classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4afea986",
      "metadata": {
        "id": "4afea986"
      },
      "source": [
        "In this second part of this lecture, you'll train a linear classifier using PyTorch.  While we showed an example of this process in class, we computed the gradients manually there (without much intuition on how that gradient was derived), and here you'll rely on PyTorch's automatic differentiation to actually compute the gradients.  Note that your implementation here will just be based upon functions, not PyTorch `Module` classes that are the more standardized way of building models in PyTorch (in the next assignment, we will use these module classes)\n",
        "\n",
        "Let's begin by first loading the required data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45f664f0",
      "metadata": {
        "id": "45f664f0"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "import torch\n",
        "\n",
        "mnist_train = datasets.MNIST(\".\", train=True, download=True)\n",
        "mnist_test = datasets.MNIST(\".\", train=False, download=True)\n",
        "X,y = mnist_train.data.reshape(60000,784)/255, mnist_train.targets\n",
        "X_test, y_test = mnist_test.data.reshape(10000,784)/255, mnist_test.targets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df67f5fd",
      "metadata": {
        "id": "df67f5fd"
      },
      "source": [
        "### Question 3 - Cross entropy loss and error\n",
        "\n",
        "Implement the following functions, which compute the cross entropy loss and the error between a set of predictions. Recall that the cross entropy loss is defined, for $\\hat{y} \\in \\mathbb{R}^k$ and $y \\in \\{1,\\ldots,k\\}$ as\n",
        "$$L_{ce}(\\hat{y}, y) = -\\log \\left ( \\frac{\\exp \\hat{y}_y}{\\sum_{j=1}^k \\exp \\hat{y}_j} \\right ) = -\\hat{y}_y + \\log \\sum_{j=1}^k \\exp \\hat{y}_j$$\n",
        "You can use the PyTorch function `torch.logsumexp` to compute the last term (this will be more numerically stable for large/small prediction values than individually calling `log` and `exp`).\n",
        "\n",
        "While you could use e.g. a for loop to compute cross entropy loss, this will be fairly inefficient later on. Instead, you should use the fact that if you index a 2D tensor with two lists of indexes, it will select the elements corresponding to each of these indexes. For instance given\n",
        "\n",
        "```python\n",
        "A = torch.tensor([[1,2,3], [4,5,6]])\n",
        "i = torch.tensor([0,1,0,1])\n",
        "j = torch.tensor([0,1,2,1])\n",
        "```\n",
        "Then\n",
        "```python\n",
        "A[i,j] = tensor([1, 5, 3, 5]) # elements [A[0,0], A[1,1], A[0,2], A[1,1]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "d06_Qmm8xzKu"
      },
      "id": "d06_Qmm8xzKu",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c4569f5f",
      "metadata": {
        "id": "c4569f5f",
        "outputId": "9349a745-0e1f-487b-81b9-4a8386a4ac05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mugrade: Running local tests for function cross_entropy_loss():...\n",
            "PASSED\n",
            "Mugrade: Running local tests for function error():...\n",
            "PASSED\n"
          ]
        }
      ],
      "source": [
        "@mugrade.local_tests\n",
        "def cross_entropy_loss(y_pred, y):\n",
        "    \"\"\"\n",
        "    Compute the average cross entropy loss between predictions and desired\n",
        "    outputs.\n",
        "\n",
        "    Input:\n",
        "        y_pred: 2D torch.Tensor[float] (N x k) - each row represents predicted\n",
        "                                                 outputs for the ith example\n",
        "        y : 1D torch.Tensor[int] (N) - each element represents desired output\n",
        "                                       of ith example\n",
        "    Output:\n",
        "        scalar torch.Tensor[float] - average cross entropy loss of the predicted\n",
        "                                     outputs\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    return (-y_pred[torch.arange(y_pred.shape[0]), y] + torch.logsumexp(y_pred, dim=1)).mean()\n",
        "    ### END YOUR CODE\n",
        "\n",
        "@mugrade.local_tests\n",
        "def error(y_pred, y):\n",
        "    \"\"\"\n",
        "    Compute the average error between predictions and desired outputs, assuming\n",
        "    we make a \"hard\" prediction of whichever class has the highest predicted\n",
        "    value.\n",
        "\n",
        "    Input:\n",
        "        y_pred: 2D torch.Tensor[float] (N x k) - each row represents predicted\n",
        "                                                 outputs for the ith example\n",
        "        y : 1D torch.Tensor[int] (N) - each element represents desired output\n",
        "                                       of ith example\n",
        "    Output:\n",
        "        scalar torch.Tensor[float] - average error of the predicted outputs\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    return (y_pred.argmax(dim=1) != y).float().mean()\n",
        "    ### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2afc5c72",
      "metadata": {
        "id": "2afc5c72"
      },
      "source": [
        "### Question 4 - (Minibatch) Stochastic Gradient descent\n",
        "\n",
        "Finally, implement a minibatch version of the stochastic gradient descent method to optimize a linear classifier, using PyTorch.  Form a linear classifier specified by a matrix $W \\in \\mathbb{R}^{k \\times n}$ (make sure to set `requires_grad=True` for this tensor, so you can compute gradients).\n",
        "\n",
        "Your function should iterate over the dataset `epochs` times, each time splitting the data into chunks of size `batch_size` (you can use the `torch.split()` function for this). For each of these chunks, compute the predictions of the linear classifier on this batch, compute the gradient of the cross entropy loss between these predictions and the desired outputs, and update $W$ by taking a step (scaled by `step_size`) in the direction of the negative gradient.  Note that after you've taken this step, you'll want to zero out the gradients of $W$ (otherwise, new gradients will be _added_ on top of the older existing gradients in the `.grad` variable, which is not what you want)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e5f3c54",
      "metadata": {
        "id": "5e5f3c54"
      },
      "outputs": [],
      "source": [
        "@mugrade.local_tests\n",
        "def train_sgd(X, y, epochs, step_size, batch_size):\n",
        "    \"\"\"\n",
        "    Run minibatch stochastic gradient descent on the dataset X,y to minimize\n",
        "    cross entropy loss.\n",
        "\n",
        "    Inputs:\n",
        "        X : 2D torch.Tensor[float] (N x n) - each row represents the ith input\n",
        "                                             of the training set\n",
        "        y : 1D torch.Tensor[int] (N) - each element represents desired output\n",
        "                                       of ith example, in 0,...,k-1\n",
        "        epochs : int - number of passes to make over the training set\n",
        "        step_size : float - step size with which to update parameters\n",
        "        batch_size : int - number of examples in a minibatch\n",
        "    Output:\n",
        "        2D torch.tensor[float] (k x n) - trained linear classifier\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    pass\n",
        "    ### END YOUR CODE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbd2553d",
      "metadata": {
        "id": "fbd2553d"
      },
      "source": [
        "If you implemented this correctly, you should be able to train a classifier using code like the following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fed74cd1",
      "metadata": {
        "id": "fed74cd1",
        "outputId": "228ee231-0c11-4cd3-e35a-d1bac4058bfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.0832)\n"
          ]
        }
      ],
      "source": [
        "W = train_sgd(X,y, step_size=0.1, epochs=5, batch_size=100)\n",
        "print(error(X_test@W.T, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "803fb6b5",
      "metadata": {
        "id": "803fb6b5"
      },
      "source": [
        "Try to play around with different step sizes, epochs, and batch sizes until you can get a classifier with error under 8% (the best we've managed is slightly less than 7.5%)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "15780",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}